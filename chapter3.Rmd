# Chapter 3: Logistic Regression

*Performing logistic regression analysis on the student alcohol consumption data*


- We'll use the data wrangled in the first part of exercise 3: the full description of the data can be found [here](https://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION)


- The data is collected on mathematics and portuguese classes: only students who answered the questionnaire on both classes have been included in the analyses.

It includes the following variables:


```{r code6, echo=T, warning = F}

# load visualization libraries

library(dplyr)
library(ggplot2)
# read the data
alc <- read.table("./data/alc_students.txt",header = T,  dec = ".")


print(colnames(alc))

```
 
 The student alcohol consumption during weekdays has been recorded in the variable Dalc, and that during weekends in Walc. The average of the two is stored in the variable alc_use. Based on this value, the stuents have been dividen in two groups: those with high alcohol use (average over 2), and the others: this is indicated in the boolean variable high_use.
 
 Next we will try to explain the high alcohol use with other variables. Now, it is not clear if he other variables cause high alcohol use, or vice versa. Still, in our model, we will explain the alcohol use using the other variables, disregarding any direction of (possible) causality. 
 
 Variables that might be related to high alcohol consumption include the student's sex, (variable sex), quality of family relationships (famrel), being in a romantic relationship (romantic), and number of school absences (absences). I would guess that, on average, males have a higher risk of high alcohol consumption. Number of school absences would possibly increase the likelihood of high alcohol use, while being in a romantic relationship and good family relationships would decrease the likelihood. Next, well explore whether these hypotheses hold true or not. 
 
 
```{r code7, echo = T}


# initialize a plot of alcohol use
g1 <- ggplot(data = alc, aes(x = sex, fill = high_use))

# define the plot as a bar plot and draw it
g1 + geom_bar() + xlab("Sex")

```
 
 In the figure we can see that, first of all, there are a few more females compared to males. However, there are almost twice as many high alcohol users within males: so this would seem to support the initial hypothesis. We will test this further with a logistic regression model in a moment. Before that, let's look at the other variables. 
 
```{r code8, echo = T}


# initialize a plot of alcohol use
g2 <- ggplot(data = alc, aes(x = famrel))

# define the plot as a bar plot and draw it
g2 + geom_histogram(binwidth = 1) + xlab("Quality of family relationships")

```
 
 We can see that most of the respondents have relatively good family relations. Now, let's check how these affect the alcohol consumption: 
 
```{r code9, echo = T}



g3 <- ggplot(alc, aes(x = high_use, y = famrel))

# define the plot as a boxplot and draw it
g3 + geom_boxplot() + ylab("Quality of family relationships")


```
 
 As we can see, the box plot is not very ideal for this task: with non-high-users, the 25th percentile and median are the same value, while for the high users the same applies for the median and 75th percentile. This shows as missing halves of the boxes. Still, we can maybe see that the high users have slightly lower scores for the quality of family relationships. Next, let's look at the romantic relationship variable, and its effect on the alcohol use.

```{r code10, echo = T}


# initialize a plot of alcohol use
g4 <- ggplot(data = alc, aes(x = romantic, fill = high_use))

# define the plot as a bar plot and draw it
g4 + geom_bar() + ylab("Being in a romantic relationship")

```

First of all, we can see that there are around twice as many people not in romantic relationship as those who are. It looks that there might be a bit smaller proportion of high alcohol users in those who are in a romantic relationship, compared to those who are not. We will see this better when we construct the model. As the last of the variables, we will look at the number of school absences. It's distribution is plotted below:

```{r code11, echo = T}


# initialize a plot of alcohol use
g5 <- ggplot(data = alc, aes(x = absences))

# define the plot as a bar plot and draw it
g5 + geom_histogram(binwidth = 1) + xlab("Number of school absences")

```
 
So most of the people have less than ten absences. Let's see how high alcohols users compare to others in this respect:
 
```{r code12, echo = T}


g6 <- ggplot(alc, aes(x = high_use, y = absences))

# define the plot as a boxplot and draw it
g6 + geom_boxplot() + ylab("Number of school absences")


```

Now we can see the box of the high alcohol users extending to clearly more absences. This is in line with the original hypothesis.

Next, let's fit the binomial model to explore the hypotheses further. 

```{r code13, echo = T}
# create the model

# find the model with glm()
m <- glm(high_use ~ famrel + romantic + absences + sex, data = alc, family = "binomial")

# print out a summary of the model
summary(m)


```

Now the high alcohol use is indicated with a true value in the variable. This means that positive coefficients signify increased likelihood of high alcohol usage, while negative coeffcients signify a decreased likelihood. As expected, good familiy relations and romantic relationship decreases the likelihood, while absences and the male sex increase it. Out of the explanatory variables, number of absences and sex are highly significant (p<0.001), and quality of family relationships is still significant (p<0.05). However, the involvment in a romantic relationship is not a statistically significant explanatory variable. So let's remove that from our model.


```{r code14, echo = T}
# create the model

# find the model with glm()
m <- glm(high_use ~ famrel  + absences + sex, data = alc, family = "binomial")

# print out a summary of the model
summary(m)


```


Now all the explanatory variables have a statistically significant relationship with the high alcohol use. To better be able to interpret the coefficients, we will convert them to odds ratios next, and also calculate their confidence intervals:



```{r code15, echo = T}
OR <- exp(coef(m))

# compute confidence intervals (CI)
CI <- exp(confint(m))

# print out the odds ratios with their confidence intervals
cbind(OR, CI)


```

The odds ratios describe how much higher the odds for a group are to belong to the high users group, compared to the previous group. So for the sex variable, it tells us that males have over twice as high odds to belong to the high users group compared to females. Similarly, with each unit increase of absences, the odds increase by 10 percent, and with each increase in the family relation quality the odds decreases by 25 percent. Now we neet to also note that the explanatory variables are on different scales: as the sex only takes two values, the difference between these two is big. The absences, on the other hand, have a much wider range of values: even if this affected the alcohol use as much, the difference caused by a unit change in the absences would of course be smaller. 

The fact that none of the confidence intervals (CIs) include one confirms that the variables are statistivally significant: if the (95 percent) CI included one, it means that it would be "possible" that a unit increase in the explanatory variable did not change the high alcohol use probability in any direction.


Next, let's look at how the predictions of the model compare to the real values:


```{r code16, echo = T}
# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)

# tabulate the target variable versus the predictions
a = table(high_use = alc$high_use, prediction = alc$prediction)
a

```

Now we classified those individuals, for which the model predicts a probability of 0.5 or more, to be predicted as high users. As we can se from the table, the model classified `r a[1,1] ` ($2^{8}$) observations, that are in reality false, to be false. In other words, these were classified correctly. `r a[1,2] ` observations that were in reality false were classified as true. Out of the observations that were in reality true, `r a[2,2] ` were classified as true, while `r a[2,1] ` were classified as false. So out of the observations that really were false, the model correctly classified the majority, while tme majority of the true-in-real-life observations were misclassified. In total, however, the model correctly classified most of the observations, in fact `r a[1,1] + a[2,2] ` out of the total `r sum(a) `. Thus, the fraction of correctly classified observations is `r round((a[1,1] + a[2,2])/sum(a)*100) ` percent. This is better than just random guessing the outcome with 50 percent probablity for each outcome. The fraction of the high users in the data is  `r round((a[2,1] + a[2,2])/sum(a)*100) ` So if we guessed for every individual that they were not heavy users, we would get 100 - `r round((a[2,1] + a[2,2])/sum(a)*100) ` = `r 100 - round((a[2,1] + a[2,2])/sum(a)*100) ` percent of the cases correct. Our model gives `r round((a[1,1] + a[2,2])/sum(a)*100) ` percent. This means that it is better than just best guess without background info on each individual, so it is of some use at least. Even if it had a lower percentage of correctly classified individuals, it could still benefit us in some way. After all, we were now only looking at binary classifications. This means that we discarded most of the information given by the model, in the form of individual probabilities for each subject.


 